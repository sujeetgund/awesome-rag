{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "IoYRtF1YWUrT"
      },
      "outputs": [],
      "source": [
        "%pip install -q langchain langchain-community faiss-cpu rank_bm25 langchain-huggingface sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "def print_docs(docs):\n",
        "    for i, doc in enumerate(docs):\n",
        "        display(Markdown(f\"**DOCUMENT {i+1}**\"))\n",
        "        print(doc.page_content)\n",
        "        print()"
      ],
      "metadata": {
        "id": "W9v14cYIXib-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.documents import Document\n",
        "\n",
        "sample_docs = [\n",
        "    Document(\n",
        "        page_content=\"LangChain is a framework for developing applications powered by large language models (LLMs).\",\n",
        "        metadata={\"source\": \"tech_overview\", \"topic\": \"frameworks\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"BM25 is a ranking function used by search engines to estimate the relevance of documents to a given search query.\",\n",
        "        metadata={\"source\": \"retrieval_docs\", \"topic\": \"algorithms\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"The Retriever-Augmented Generation (RAG) architecture combines retrieval systems with generative models to reduce hallucinations.\",\n",
        "        metadata={\"source\": \"rag_theory\", \"topic\": \"architecture\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Sparse retrievers like TF-IDF and BM25 rely on exact keyword matching and term frequency-inverse document frequency (TF-IDF) logic.\",\n",
        "        metadata={\"source\": \"retrieval_docs\", \"topic\": \"sparse_retrieval\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Dense retrievers use vector embeddings to represent semantic meaning, often using models like Sentence-BERT or OpenAI's text-embedding-3.\",\n",
        "        metadata={\"source\": \"retrieval_docs\", \"topic\": \"dense_retrieval\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Hybrid search combines the results of sparse and dense retrievers to leverage both keyword accuracy and semantic context.\",\n",
        "        metadata={\"source\": \"hybrid_search_guide\", \"topic\": \"optimization\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Vector databases like Pinecone, Milvus, and Weaviate are commonly used to store and query high-dimensional embeddings.\",\n",
        "        metadata={\"source\": \"database_docs\", \"topic\": \"vector_stores\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Prompt engineering involves crafting specific instructions to guide an LLM toward producing desired outputs.\",\n",
        "        metadata={\"source\": \"prompt_guide\", \"topic\": \"prompt_engineering\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"RecursiveCharacterTextSplitter is a popular LangChain tool for breaking down long documents into smaller, manageable chunks.\",\n",
        "        metadata={\"source\": \"preprocessing_docs\", \"topic\": \"chunking\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Reranking is a post-retrieval step where a secondary model re-evaluates the relevance of the top-k retrieved documents.\",\n",
        "        metadata={\"source\": \"optimization_docs\", \"topic\": \"reranking\"}\n",
        "    )\n",
        "]"
      ],
      "metadata": {
        "id": "V8bGvox4Wlvi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.retrievers import BM25Retriever\n",
        "\n",
        "bm25_retriever = BM25Retriever.from_documents(sample_docs)\n",
        "bm25_retriever.k = 5"
      ],
      "metadata": {
        "id": "cmMjCigIxLGF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "embedding_fn = HuggingFaceEmbeddings(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "vectorstore = FAISS.from_documents(sample_docs, embedding=embedding_fn)"
      ],
      "metadata": {
        "id": "epOyG0J8XJYp"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})"
      ],
      "metadata": {
        "id": "V2zxn3RvvALB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_classic.retrievers import EnsembleRetriever\n",
        "\n",
        "retriever = EnsembleRetriever(\n",
        "    retrievers=[bm25_retriever, vector_retriever],\n",
        "    weights=[0.35, 0.65]\n",
        ")"
      ],
      "metadata": {
        "id": "TY6LWw20xUL6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = 'How does BM25 work with sparse retrievers?'\n",
        "\n",
        "results = retriever.invoke(query)\n",
        "print_docs(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "AHyU3MaVXWLU",
        "outputId": "c56b5fd5-f5de-4523-aec6-fe5d8667c90f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**DOCUMENT 1**"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sparse retrievers like TF-IDF and BM25 rely on exact keyword matching and term frequency-inverse document frequency (TF-IDF) logic.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**DOCUMENT 2**"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hybrid search combines the results of sparse and dense retrievers to leverage both keyword accuracy and semantic context.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**DOCUMENT 3**"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BM25 is a ranking function used by search engines to estimate the relevance of documents to a given search query.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**DOCUMENT 4**"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dense retrievers use vector embeddings to represent semantic meaning, often using models like Sentence-BERT or OpenAI's text-embedding-3.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**DOCUMENT 5**"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reranking is a post-retrieval step where a secondary model re-evaluates the relevance of the top-k retrieved documents.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**DOCUMENT 6**"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Retriever-Augmented Generation (RAG) architecture combines retrieval systems with generative models to reduce hallucinations.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**DOCUMENT 7**"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RecursiveCharacterTextSplitter is a popular LangChain tool for breaking down long documents into smaller, manageable chunks.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}