{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "IoYRtF1YWUrT"
      },
      "outputs": [],
      "source": [
        "%pip install -q langchain langchain-community langchain-groq faiss-cpu rank_bm25 langchain-huggingface sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "def print_docs(docs):\n",
        "    for i, doc in enumerate(docs):\n",
        "        display(Markdown(f\"**DOCUMENT {i+1}**\"))\n",
        "        print(doc.page_content)\n",
        "        print()"
      ],
      "metadata": {
        "id": "W9v14cYIXib-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.documents import Document\n",
        "\n",
        "sample_docs = [\n",
        "    Document(\n",
        "        page_content=\"LangChain is a framework for developing applications powered by large language models (LLMs).\",\n",
        "        metadata={\"source\": \"tech_overview\", \"topic\": \"frameworks\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"BM25 is a ranking function used by search engines to estimate the relevance of documents to a given search query.\",\n",
        "        metadata={\"source\": \"retrieval_docs\", \"topic\": \"algorithms\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"The Retriever-Augmented Generation (RAG) architecture combines retrieval systems with generative models to reduce hallucinations.\",\n",
        "        metadata={\"source\": \"rag_theory\", \"topic\": \"architecture\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Sparse retrievers like TF-IDF and BM25 rely on exact keyword matching and term frequency-inverse document frequency (TF-IDF) logic.\",\n",
        "        metadata={\"source\": \"retrieval_docs\", \"topic\": \"sparse_retrieval\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Dense retrievers use vector embeddings to represent semantic meaning, often using models like Sentence-BERT or OpenAI's text-embedding-3.\",\n",
        "        metadata={\"source\": \"retrieval_docs\", \"topic\": \"dense_retrieval\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Hybrid search combines the results of sparse and dense retrievers to leverage both keyword accuracy and semantic context.\",\n",
        "        metadata={\"source\": \"hybrid_search_guide\", \"topic\": \"optimization\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Vector databases like Pinecone, Milvus, and Weaviate are commonly used to store and query high-dimensional embeddings.\",\n",
        "        metadata={\"source\": \"database_docs\", \"topic\": \"vector_stores\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Prompt engineering involves crafting specific instructions to guide an LLM toward producing desired outputs.\",\n",
        "        metadata={\"source\": \"prompt_guide\", \"topic\": \"prompt_engineering\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"RecursiveCharacterTextSplitter is a popular LangChain tool for breaking down long documents into smaller, manageable chunks.\",\n",
        "        metadata={\"source\": \"preprocessing_docs\", \"topic\": \"chunking\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Reranking is a post-retrieval step where a secondary model re-evaluates the relevance of the top-k retrieved documents.\",\n",
        "        metadata={\"source\": \"optimization_docs\", \"topic\": \"reranking\"}\n",
        "    )\n",
        "]"
      ],
      "metadata": {
        "id": "V8bGvox4Wlvi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.retrievers import BM25Retriever\n",
        "\n",
        "bm25_retriever = BM25Retriever.from_documents(sample_docs)\n",
        "bm25_retriever.k = 5"
      ],
      "metadata": {
        "id": "cmMjCigIxLGF"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "embedding_fn = HuggingFaceEmbeddings(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "vectorstore = FAISS.from_documents(sample_docs, embedding=embedding_fn)"
      ],
      "metadata": {
        "id": "epOyG0J8XJYp"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})"
      ],
      "metadata": {
        "id": "V2zxn3RvvALB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_classic.retrievers import MergerRetriever\n",
        "\n",
        "merger_retriever = MergerRetriever(\n",
        "    retrievers=[bm25_retriever, vector_retriever]\n",
        ")"
      ],
      "metadata": {
        "id": "TY6LWw20xUL6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from google.colab import userdata\n",
        "\n",
        "llm = ChatGroq(\n",
        "    model=\"openai/gpt-oss-120b\",\n",
        "    api_key=userdata.get('GROQ_API_KEY')\n",
        ")"
      ],
      "metadata": {
        "id": "4Hq0Bbbs5S2J"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "query_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Generate 3 different search queries separated by new lines to retrieve relevant documents for: {question}\"\n",
        ")"
      ],
      "metadata": {
        "id": "dal3_CCH49uH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "sample_chain = (\n",
        "    query_prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        "    | (lambda x: x.split(\"\\n\"))\n",
        ")\n",
        "\n",
        "sample_chain.invoke({\"question\": \"How does BM25 work with sparse retrievers?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NbgclroRzafP",
        "outputId": "ac9315d5-7676-44b7-ee6c-6953ead60545"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['BM25 sparse retrieval mechanism and its integration with modern sparse retrievers  ',\n",
              " 'How BM25 scoring is applied in sparse neural retrievers for information retrieval  ',\n",
              " 'Combining traditional BM25 with sparse embedding models for document ranking']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "query_gen_chain = (\n",
        "    query_prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        "    | (lambda x: x.split(\"\\n\"))\n",
        ")"
      ],
      "metadata": {
        "id": "Lh7tUBUPC-QR"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reciprocal_rank_fusion(results: list[list], k=60):\n",
        "    \"\"\"\n",
        "    Combines multiple lists of Documents into one re-ranked list.\n",
        "    Formula: score = sum(1 / (rank + k))\n",
        "    \"\"\"\n",
        "    fused_scores = {}\n",
        "\n",
        "    for docs in results:\n",
        "        for rank, doc in enumerate(docs):\n",
        "            # We use page_content as a unique key (or doc.metadata['id'] if available)\n",
        "            doc_content = doc.page_content\n",
        "            if doc_content not in fused_scores:\n",
        "                fused_scores[doc_content] = {\"doc\": doc, \"score\": 0}\n",
        "\n",
        "            # Apply the RRF formula\n",
        "            fused_scores[doc_content][\"score\"] += 1 / (rank + k)\n",
        "\n",
        "    # Sort documents by their fused score in descending order\n",
        "    reranked_results = sorted(\n",
        "        fused_scores.values(),\n",
        "        key=lambda x: x[\"score\"],\n",
        "        reverse=True\n",
        "    )\n",
        "\n",
        "    # Return only the Document objects\n",
        "    return [item[\"doc\"] for item in reranked_results]"
      ],
      "metadata": {
        "id": "t6V1nqUGDgYy"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "rag_fusion_chain = (\n",
        "    {\"question\": RunnablePassthrough()}\n",
        "    | query_gen_chain\n",
        "    | merger_retriever.map()\n",
        "    | reciprocal_rank_fusion\n",
        ")"
      ],
      "metadata": {
        "id": "AzUtpMTNDn_o"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = 'How does BM25 work with sparse retrievers?'\n",
        "\n",
        "results = rag_fusion_chain.invoke({\"question\": question})\n",
        "print_docs(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 593
        },
        "id": "AHyU3MaVXWLU",
        "outputId": "ff2ff0b8-4e78-4ef6-c72e-82e9f063789d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**DOCUMENT 1**"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sparse retrievers like TF-IDF and BM25 rely on exact keyword matching and term frequency-inverse document frequency (TF-IDF) logic.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**DOCUMENT 2**"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hybrid search combines the results of sparse and dense retrievers to leverage both keyword accuracy and semantic context.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**DOCUMENT 3**"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BM25 is a ranking function used by search engines to estimate the relevance of documents to a given search query.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**DOCUMENT 4**"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Retriever-Augmented Generation (RAG) architecture combines retrieval systems with generative models to reduce hallucinations.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**DOCUMENT 5**"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reranking is a post-retrieval step where a secondary model re-evaluates the relevance of the top-k retrieved documents.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**DOCUMENT 6**"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dense retrievers use vector embeddings to represent semantic meaning, often using models like Sentence-BERT or OpenAI's text-embedding-3.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**DOCUMENT 7**"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector databases like Pinecone, Milvus, and Weaviate are commonly used to store and query high-dimensional embeddings.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**DOCUMENT 8**"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RecursiveCharacterTextSplitter is a popular LangChain tool for breaking down long documents into smaller, manageable chunks.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**DOCUMENT 9**"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LangChain is a framework for developing applications powered by large language models (LLMs).\n",
            "\n"
          ]
        }
      ]
    }
  ]
}